{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy               \n",
    "\n",
    "'''pip insall Scrapy'''\n",
    "'''scrapy startproject jobscrapper to create a new project in the command line'''\n",
    "class JobOpeningItem(scrapy.Item):\n",
    "    id = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    company = scrapy.Field()\n",
    "    department = scrapy.Field()\n",
    "    location = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "    requirements = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"only print out title after exiting the Pipeline\"\"\"\n",
    "        return repr({\"title\": self['title']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recruitee_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jobscrapper\n",
    "\n",
    "'''pip insall jobs-scrapper'''\n",
    "\n",
    "class RecruiteeSpider(scrapy.Spider):\n",
    "    name = \"recruitee\"\n",
    "\n",
    "    start_urls = [\n",
    "        # Replace with your list of recruitee-backed job pages\n",
    "        'https://someone.recruitee.com/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        company = response.url[8:].split('.')[0]\n",
    "        jobs = response.css('div.job')\n",
    "        for job in jobs:\n",
    "            yield scrapy.Request(url=response.urljoin(job.css('h5.job-title a::attr(href)')[0].extract()),\n",
    "                                callback=self.parse_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recruitee.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import jobscrapper\n",
    "\n",
    "class RecruiteeSpider(scrapy.Spider):\n",
    "    name = \"recruitee\"\n",
    "\n",
    "    start_urls = [\n",
    "        # Replace with your list of recruitee-backed job pages\n",
    "        'https://someone.recruitee.com/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        company = response.url[8:].split('.')[0]\n",
    "        jobs = response.css('div.job')\n",
    "        for job in jobs:\n",
    "            yield scrapy.Request(url=response.urljoin(job.css('h5.job-title a::attr(href)')[0].extract()),\n",
    "                                 meta={\n",
    "                                     'company': company,\n",
    "                                     'department': job.css('div.department::text').get(),\n",
    "                                     'location': job.css('li.job-location::text').get(),\n",
    "                                 }, callback=self.parse_job)\n",
    "\n",
    "    def parse_job(self, response):\n",
    "        yield JobOpeningItem(\n",
    "            id=response.meta['id'],\n",
    "            title=response.css('h2.title::text').get(),\n",
    "            company=response.meta['company'],\n",
    "            department=response.meta['department'],\n",
    "            location=response.meta['location'],\n",
    "            description='\\n'.join(response.xpath('//div[@class=\"description\"]')[0].xpath('./*').getall()),\n",
    "            requirements='\\n'.join(response.xpath('//div[@class=\"description\"]')[1].xpath('./*').getall()),\n",
    "            link=response.url\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# workable_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import scrapy.http\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import jobscrapper\n",
    "\n",
    "WORKABLE_API_URL = 'https://careers-page.workable.com'\n",
    "WORKABLE_ACCOUNTS_API_URL = urljoin(WORKABLE_API_URL, 'api/v3/accounts')\n",
    "WORKABLE_JOB_API_URL = urljoin(WORKABLE_API_URL, 'api/v2/accounts')\n",
    "\n",
    "\n",
    "class WorkableSpider(scrapy.Spider):\n",
    "    name = \"workable\"\n",
    "    start_urls = [\n",
    "        # Replace with your list of URLS\n",
    "        'https://apply.workable.com/someone/',\n",
    "    ]\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            company = url.split('/')[-2]\n",
    "            yield scrapy.Request(url=f'{WORKABLE_ACCOUNTS_API_URL}/{company}/jobs',\n",
    "                                 meta={'base_url': url},\n",
    "                                 method='POST',\n",
    "                                 callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        company = response.url.split(\"/\")[-2]\n",
    "        data = response.json()\n",
    "        for job in data.get('results', []):\n",
    "            yield scrapy.Request(url=f'{WORKABLE_JOB_API_URL}/{company}/jobs/{job[\"shortcode\"]}',\n",
    "                                 meta={\n",
    "                                     **job,\n",
    "                                     **response.meta,\n",
    "                                     'company': company\n",
    "                                 },\n",
    "                                 callback=self.parse_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# workable_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job(self, response):\n",
    "        job = response.json()\n",
    "        yield JobOpeningItem(\n",
    "            title=job.get('title'),\n",
    "            company=response.meta['company'],\n",
    "            department=job.get('department')[0] if job.get('department') else '',\n",
    "            location=job.get('location', {}).get('city'),\n",
    "            description=job.get('description'),\n",
    "            requirements=job.get('requirements'),\n",
    "            link=urljoin(response.meta['base_url'], f'j/{response.meta[\"shortcode\"]}')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# workable_parse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def parse(self, response):\n",
    "        company = response.url.split(\"/\")[-2]\n",
    "        data = response.json()\n",
    "        for job in data.get('results', []):\n",
    "            yield scrapy.Request(url=f'{WORKABLE_JOB_API_URL}/{company}/jobs/{job[\"shortcode\"]}',\n",
    "                                 meta={\n",
    "                                     **job,\n",
    "                                     **response.meta,\n",
    "                                     'company': company\n",
    "                                 },\n",
    "                                 callback=self.parse_job)\n",
    "\n",
    "        # Handle paging\n",
    "        if data.get('nextPage'):\n",
    "            yield scrapy.http.JsonRequest(url=f'{WORKABLE_ACCOUNTS_API_URL}/{company}/jobs',\n",
    "                                          meta=response.meta,\n",
    "                                          data={\"token\": data['nextPage'], \"query\": \"\", \"location\": [],\n",
    "                                                \"department\": [],\n",
    "                                                \"worktype\": [], \"remote\": []},\n",
    "                                          callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobOpeningsPipeline:\n",
    "    def _clean(self, value):\n",
    "        if value:\n",
    "            return ' '.join(value.split())\n",
    "\n",
    "        return value\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        item = JobOpeningItem(item)\n",
    "        for field in ('title', 'location', 'department'):\n",
    "            item[field] = self._clean(item[field])\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'jobscrapper.pipelines.JobOpeningsPipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Run Recruitee spider\\nscrapy runspider openings/spiders/recruitee.py -o jobs.json\\n# Run Workable spider\\nscrapy runspider openings/spiders/workable.py -o jobs.json'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Run Recruitee spider\n",
    "scrapy runspider openings/spiders/recruitee.py -o jobs.json\n",
    "# Run Workable spider\n",
    "scrapy runspider openings/spiders/workable.py -o jobs.json'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# piplines_exporter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itemadapter import ItemAdapter\n",
    "from scrapy.exporters import JsonItemExporter\n",
    "\n",
    "\n",
    "class PerCompanyExportPipeline:\n",
    "    \"\"\"Distribute items across multiple XML files according to their 'year' field here it is used to store different companies\"\"\"\n",
    "\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        settings = crawler.settings\n",
    "        return cls(settings.get('JOBS_PATH', '.'))\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.company_to_exporter = {}\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        for exporter in self.company_to_exporter.values():\n",
    "            exporter.finish_exporting()\n",
    "\n",
    "    def _exporter_for_item(self, item):\n",
    "        adapter = ItemAdapter(item)\n",
    "        company = adapter['company']\n",
    "        if company not in self.company_to_exporter:\n",
    "            f = open(os.path.join(self.path, f'{company}.json'), 'wb')\n",
    "            exporter = JsonItemExporter(f, indent=4)\n",
    "            exporter.start_exporting()\n",
    "            self.company_to_exporter[company] = exporter\n",
    "        return self.company_to_exporter[company]\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        exporter = self._exporter_for_item(item)\n",
    "        exporter.export_item(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOBS_PATH = 'data'\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'jobscrapper.pipelines.JobOpeningsPipeline': 300,\n",
    "    'jobscrapper.pipelines.PerCompanyExportPipeline': 600,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Run Recruitee spider\\nscrapy runspider openings/spiders/recruitee.py\\n# Run Workable spider\\nscrapy runspider openings/spiders/workable.py'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Run Recruitee spider\n",
    "scrapy runspider openings/spiders/recruitee.py\n",
    "# Run Workable spider\n",
    "scrapy runspider openings/spiders/workable.py'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name: Scrape latest data\n",
    "\n",
    "on:\n",
    "  push:\n",
    "  workflow_dispatch:\n",
    "  schedule:\n",
    "    - cron:  '0 13 * * 1'\n",
    "\n",
    "jobs:\n",
    "  scheduled:\n",
    "    runs-on: ubuntu-latest\n",
    "    strategy:\n",
    "      matrix:\n",
    "        python-version: [ 3.8 ]\n",
    "\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python ${{ matrix.python-version }}\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: ${{ matrix.python-version }}\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n",
    "    - name: Fetch latest data\n",
    "      run: |-\n",
    "        scrapy runspider jobscrapper/spiders/workable.py && scrapy runspider jobscrapper/spiders/recruitee.py\n",
    "    - name: Commit and push if it changed\n",
    "      run: |-\n",
    "        git config user.name \"Automated\"\n",
    "        git config user.email \"actions@users.noreply.github.com\"\n",
    "        git add -A\n",
    "        timestamp=$(date -u)\n",
    "        git commit -m \"Latest data: ${timestamp}\" || exit 0\n",
    "        git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
